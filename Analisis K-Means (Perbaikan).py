# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k57Gs-spmgedWjC62WoNgN5FR1r2-KqX
"""

#Title :Clustering Students Based on Lifestyle
#Dataset : Student Lifestyle Dataset from kaggle

!pip install openpyxl

# Import necessary libraries
import os
import pandas as pd
import numpy as np
import seaborn as sns
import warnings
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Suppress FutureWarnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Load the dataset
file_path = "data lifestyle student.xlsx"
# Use the 'engine' parameter to specify openpyxl for reading xlsx files
data = pd.read_excel(file_path, engine='openpyxl')

# Display the first few rows of the dataset
data.head()

# Get basic information about the dataset
data.info()

# Check for missing values
print("Missing values:\n", data.isnull().sum())

#Data Visualization (untuk memvisualisasikan pola dan hubungan antara variabel yang berbeda)
#Menggunakan dua metode visualisasi: Heatmap dan Pair Plot, untuk mengeksplorasi hubungan antar fitur.
# 1. Heatmap menunjukkan korelasi antar variabel numerik, membantu mengidentifikasi hubungan seperti korelasi kuat, lemah, atau tidak ada korelasi.
# 2. Pair Plot menyediakan scatter plots and distribution plots untuk memeriksa pola, cluster, atau tren data secara visual.


# Select only numeric columns
numeric_data = data.select_dtypes(include=['float64', 'int64'])

# Plot a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(numeric_data.corr(), annot=True, cmap='viridis', fmt=".2f")
plt.title('Matriks Koreasidari Variabel Students Lifestyle')
plt.show()


# Pair plot to visualize relationships
sns.pairplot(data, diag_kind='kde')
plt.suptitle('Pair Plot of Variable', y=1.02)
plt.show()

#Hasil dan Analisis:
##----Untuk Heatmap
# - Heatmap mengungkapkan pola korelasi negatif antara "Study_Hours_Per_Day" and "Social_Hours_Per_Day," menunjukkan bahwa lebih banyak jam belajar mungkin mengurangi jam sosial.
# - Variabel seperti "Sleep_Hours_Per_Day" and "Physical_Activity_Hours_Per_Day" show weak or no correlation, artinya mereka independent.
##----Untuk Pair Plot
# - Pair Plot menyediakan scatter plots untuk menyoroti potensi cluster atau tren dalam hubungan variabel.
# - The diagonal KDE plots menunjukkan feature distributions, revealing skewness or normality.
# - visualisasi ini menyediakan scatter plot untuk menyoroti potensi cluster atau tren dalam hubungan variabel.

#Data Preprocessing
#Mempersiapkan data untuk clustering dengan memilih variabel-variabel penting dan men-skalakannya.
# 1. Pilih variabel numerik yang terkait dengan student lifestyle untuk clustering.
# 2. Standarisasi data sehingga semua variabel berada pada skala yang sama. Hal ini penting karena clustering berfungsi lebih baik dengan data berskala.
# 3.Konversikan kembali data yang diskalakan menjadi DataFrame agar lebih mudah digunakan nanti.

# Select numeric columns untuk clustering
variables = ['Jam_Belajar_Per_Hari', 'Jam_Ekstrakurikuler_Per_Hari',
            'Jam_Tidur_Per_Hari', 'Jam_Sosial_Per_Hari', 'Jam_Aktivitas_Fisik_Per_Hari']
data_clustering = data[variables]

# Convert columns to numeric, errors='coerce' will replace non-numeric values with NaN
for col in variables:
    data_clustering[col] = pd.to_numeric(data_clustering[col], errors='coerce')

# Drop rows with NaN values
data_clustering = data_clustering.dropna()

# Standardize the variables
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_clustering)

# Check if the data is standardized
print("Mean of each variable after scaling:")
print(data_scaled.mean(axis=0))  # Add axis=0 untuk menghitung mean masing-masing variabel

print("\nStandard deviation of each variable after scaling:")
print(data_scaled.std(axis=0))  # Add axis=0 untuk menghitung standard deviation masing-masing variabel

# Convert back to DataFrame for easier handling
data_scaled = pd.DataFrame(data_scaled, columns=variables)

# Results and analysis:
# - Variabel - variabelnya telah berhasil diskalakan, dengan rata-rata mendekati 0 dan deviasi standar mendekati 1.
# - Data sekarang telah dipersiapkan dengan baik untuk clustering.

# Variable Extraction
# Gunakan Principal Component Analysis (PCA) untuk mengurangi kumpulan data menjadi dua dimensi untuk visualisasi yang lebih baik sambil mempertahankan varian maksimum.
# - PCA digunakan untuk mengurangi dimensi kumpulan data sekaligus menjaga informasi (varians) sebanyak mungkin.
# - Hal ini membantu dalam memvisualisasikan kumpulan data dalam 2D, yang sangat berguna untuk memahami hasil pengelompokan.

# Apply PCA untuk mengurangi dimensi
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)

# Add PCA results to the DataFrame
data_scaled['PCA1'] = data_pca[:, 0]
data_scaled['PCA2'] = data_pca[:, 1]

# Apply Machine Learning Techniques
# Melakukan clustering menggunakan K-Means dengan variasi parameter untuk mengoptimalkan hasil.
# 1. Gunakan Elbow Method untuk menentukan jumlah cluster yang optimal dengan memplot inersia (jumlah kuadrat dalam cluster) untuk nilai k yang berbeda.
# 2. Apply the K-Means algorithm dengan jumlah cluster yang optimal untuk mengelompokkan data berdasarkan kemiripan.

# Determine the optimal number of clusters using the Elbow Method
inertia = []  # Daftar untuk menyimpan inersia untuk setiap nilai k
for k in range(1, 11):  # Test cluster sizes from 1 to 10
    kmeans = KMeans(n_clusters=k, random_state=42)  # Initialize K-Means
    kmeans.fit(data_scaled.iloc[:, :-2])  # Fit K-Means excluding PCA columns
    inertia.append(kmeans.inertia_)  # Store the inertia

# Plot the elbow curve
plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), inertia, marker='o')  # Plot inertia for each value of k
plt.title('Elbow Method for Optimal Clusters')  # Title of the plot
plt.xlabel('Number of Clusters')  # X-axis label
plt.ylabel('Inertia')  # Y-axis label
plt.show()


# Apply K-Means with the optimal number of clusters
optimal_clusters = 3  # Set the optimal number of clusters based on the elbow method
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)  # Initialize K-Means
data_scaled['Cluster'] = kmeans.fit_predict(data_scaled.iloc[:, :-2])  # Assign clusters


#Hasil dari Elbow Method:
#Grafik menunjukkan penurunan inersia yang tajam dari k=1 ke k=3, yang menunjukkan bahwa peningkatan jumlah cluster secara signifikan mengurangi variasi dalam cluster.
#Setelah k=3, kurva mulai mendatar, yang berarti manfaat penambahan lebih banyak cluster menjadi minimal.
#Jadi, k=3 adalah elbow point, menjadikannya jumlah cluster yang optimal untuk kumpulan data ini.

#Hasil dari (K-Means Clustering):
# - Setiap titik data telah ditetapkan ke salah satu dari 3 cluster.

#experimentation
# Visualisasi Clusters menggunakan PCA
plt.figure(figsize=(8, 6))
sns.scatterplot(data=data_scaled, x='PCA1', y='PCA2', hue='Cluster', palette='Dark2', s=100)
plt.title('Visualisasi Cluster (PCA Reduced)')
plt.show()

#Visualisasinya menunjukkan pemisahan yang jelas antar kelompok, yang menunjukkan pola student lifestyle yang berbeda antar kelompok.

# Hasil Analisis
# Analisis karakteristik dan ukuran setiap cluster untuk memahami pola dari student lifestyles.
# 1. Hitung nilai rata-rata variabel dalam setiap cluster untuk memahami karakteristik unik setiap grup.
# 2. Hitung jumlah siswa di setiap cluster untuk melihat bagaimana kumpulan data didistribusikan ke seluruh kelompok.
# 3. Gunakan informasi ini untuk menafsirkan perilaku dan pola gaya hidup siswa di setiap cluster.

# Apply Machine Learning Techniques
# Melakukan clustering menggunakan K-Means dengan variasi parameter untuk mengoptimalkan hasil.
# 1. Gunakan Elbow Method untuk menentukan jumlah cluster yang optimal dengan memplot inersia (jumlah kuadrat dalam cluster) untuk nilai k yang berbeda.
# 2. Apply the K-Means algorithm dengan jumlah cluster yang optimal untuk mengelompokkan data berdasarkan kemiripan.

# Analyze cluster characteristics
cluster_profiles = data_scaled.groupby('Cluster').mean()

# Rename columns for better readability
cluster_profiles.rename(
    columns={
        "Jam_Belajar_Per_Hari": "Jam Belajar (Avg)",
        "Jam_Ekstrakurikuler_Per_Hari": "Jam Ekstrakurikuler (Avg)",
        "Jam_Tidur_Per_Hari": "Jam Tidur (Avg)",
        "Jam_Sosial_Per_Hari": "Jam Sosial (Avg)",
        "Jam_Aktivitas_Fisik_Per_Hari": "Jam Aktivitas Fisik (Avg)",
    },
    inplace=True,
)

# Display cluster characteristics
print("\nCluster Characteristics (Average Values):\n")
for cluster, profile in cluster_profiles.iterrows():
    print(f"Cluster {cluster}:")
    for variables, value in profile.items():
        print(f"  {variables}: {value:.4f}")
    print()  # Blank line for separation

# Count the number of students in each cluster
cluster_sizes = data_scaled['Cluster'].value_counts()

# Display cluster sizes
print("Cluster Sizes:\n")
for cluster, size in cluster_sizes.items():
    print(f"  Cluster {cluster}: {size} students")

# Penjelasan hasil dan analisis:
# - Nilai rata-rata variabel di setiap cluster menunjukkan pola students lifestyle yang berbeda.
# - Cluster 0: Gaya hidup seimbang, dengan rata-rata belajar dan aktivitas fisik tetapi jam sosial yang tinggi.
# - Cluster 1: Siswa yang fokus secara akademis, dengan jam belajar dan tidur tinggi tetapi aktivitas fisik dan sosial rendah.
# - Cluster 2: Gaya hidup aktif, dengan aktivitas fisik yang tinggi tetapi jam belajar dan sosial yang lebih rendah.
# - Ukuran cluster menunjukkan bagaimana siswa didistribusikan, dengan Cluster 0 menjadi kelompok terbesar.

# Periksa kolom Cluster
print(data_scaled.head())

# Periksa nilai unik pada kolom Cluster
print(data_scaled['Cluster'].unique())

# Hitung jumlah siswa di setiap kluster
cluster_sizes = data_scaled['Cluster'].value_counts()
print(cluster_sizes)

# Verifikasi total siswa
print("Total siswa:", cluster_sizes.sum())

from sklearn.cluster import KMeans

# Contoh klustering
kmeans = KMeans(n_clusters=3, random_state=42)
data_scaled['Cluster'] = kmeans.fit_predict(data_scaled)

# Periksa ulang
print(data_scaled['Cluster'].value_counts())